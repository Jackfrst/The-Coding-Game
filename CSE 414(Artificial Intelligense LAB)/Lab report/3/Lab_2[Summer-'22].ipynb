{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing K-NN:\n",
    "\n",
    "1. অজানা ডেটা পয়েন্ট থেকে বাকি সব ডেটা পয়েন্টের দূরত্ব বের করতে হবে\n",
    "2. দূরত্বের মান অনুযায়ী ছোট থেকে বড় আকারে (ascending order) ডেটা পয়েন্টগুলো সর্ট (sorting) করে নিতে হবে\n",
    "3. সর্ট করা পয়েন্ট থেকে প্রথম k-সংখ্যক পয়েন্ট নিতে হবে।\n",
    "4. এই k-সংখ্যক ডেটা পয়েন্টের মধ্যে যে ক্লাসের পয়েন্ট সবচে বেশি সংখ্যকবার আছে,অজানা ডেটা পয়েন্টটিকে সেই ক্লাস হিসেবে চিহ্নিত করতে হবে।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "from collections import Counter\n",
    "style.use('fivethirtyeight')\n",
    "\n",
    "dataset = {\n",
    "        'k':[[50,110],[65,130],[70,140],[55,120],[85,138],[88,142],[87,144],[89,141],[75,135],[80,140],[55,120]],\n",
    "        'r':[[50,110],[65,130],[70,140],[55,120],[85,138],[88,142],[87,144],[89,141],[75,135],[80,140],[55,120]],\n",
    "        }\n",
    "\n",
    "def scatter_plot(new_feat):\n",
    "    # In dataset there are 2 type data'k' & 'r'\n",
    "    # so this loop is use to choice each element\n",
    "    for i in dataset:\n",
    "\n",
    "        # now we know that , i = k or r\n",
    "        # so now it will take the arry that under the 'k' variable\n",
    "        # so here dataset[i] is a 2D array, and ii is a each element of 2D array\n",
    "        for ii in dataset[i]:\n",
    "            # now ii is a 2D array so that we will plot one by one!!!\n",
    "            plt.scatter(ii[0], ii[1], s=100, color=i)\n",
    "\n",
    "    # we need to plot new feature also! so its here\n",
    "    plt.scatter(new_feat[0],new_feat[1],color='b')\n",
    "    # plt.show for show  the graph\n",
    "    plt.show()\n",
    "    \n",
    "def k_NN(data,predict,k=7):\n",
    "    distances = []\n",
    "    # again we will take the element here k or r\n",
    "    for group in data:\n",
    "\n",
    "        # we will go inside each 2D array\n",
    "        for features in data[group]:\n",
    "            # here we measure the distance\n",
    "            euclidian_distances=np.linalg.norm(np.array(features)-np.array(predict))\n",
    "            # all the distence adding into a array\n",
    "            distances.append([euclidian_distances,group])\n",
    "\n",
    "\n",
    "            # take votes for near by element\n",
    "    #list comprehension(consists of: output expression,input sequence & coditional logic)\n",
    "    votes = [i[1] for i in sorted(distances)[:k]] \n",
    "    #for better perception,print all them separately and brood over\n",
    "    print(Counter(votes).most_common(1))\n",
    "    vote_result = Counter(votes).most_common(1)[0][0]\n",
    "    return vote_result\n",
    "\n",
    "while True:\n",
    "    # we are taking 2 inputs by adding them with a ',' so that we splite that string, for getting seperate input\n",
    "    new_arr = input('Add new sample [weight,height]= ').split(',')\n",
    "\n",
    "    # on this array we will take our input\n",
    "    new_feat = []\n",
    "    new_feat.append(int(new_arr[0]))\n",
    "    new_feat.append(int(new_arr[1]))\n",
    "\n",
    "    if new_feat==[0,0]:\n",
    "        break\n",
    "    else:\n",
    "        print(new_feat)\n",
    "        scatter_plot(new_feat)\n",
    "        result = k_NN(dataset,new_feat,k=7)\n",
    "        print('Result',result)\n",
    "        if result=='k':\n",
    "            print('Footballer')\n",
    "        else:\n",
    "            print('Wrestler')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-graphical Exploratory Data Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris=datasets.load_iris()\n",
    "#print(iris)\n",
    "#dir(iris)\n",
    "#print(iris.DESCR)\n",
    "print(iris.data.shape)\n",
    "print(iris.feature_names)\n",
    "#print(type(iris))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(iris.keys())\n",
    "print(iris.DESCR)\n",
    "print(iris.feature_names)\n",
    "print(iris.data)\n",
    "print(iris.target_names)\n",
    "print(iris.target)\n",
    "print(iris.data.shape)\n",
    "print(iris.target.shape)\n",
    "X=iris.data\n",
    "Y=iris.target\n",
    "\n",
    "display(X)\n",
    "display(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphical ED Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "iris_data=sns.load_dataset('iris')\n",
    "#print(iris_data)\n",
    "#dir(iris_data)\n",
    "correlation=iris_data.corr()\n",
    "#print(correlation)\n",
    "sns.heatmap(correlation,cmap='winter',annot=True)\n",
    "\n",
    "#try using boxplot,pairplot,Implot by reading seaborn documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing a dataset of my own using pandas(here my dataset is a csv(comma separated value) file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#here 'mock.csv is my mock dataset file name that I've kept under the 'files' tab of jupyter notebook dashboard'\n",
    "df=pd.read_csv('mock.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buidling classification models using pycaret:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pycaret\n",
    "from pycaret.datasets import get_data\n",
    "a=get_data('index')\n",
    "#print(a)\n",
    "iris=get_data('iris')\n",
    "from pycaret.classification import *\n",
    "exp=setup(iris,target='species')\n",
    "compare_models()\n",
    "\n",
    "#try building the models other datasets and dataset of your own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8496b14b7b0a9aed46168eced9533ed7949e07e394b13acc00d587bdb6fd8302"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
